{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost val: 1041.878662109375\n",
      "cost val: 651.8825073242188\n",
      "cost val: 124.14537048339844\n",
      "cost val: 27.52798843383789\n",
      "cost val: 102.80746459960938\n",
      "cost val: 89.76568603515625\n",
      "cost val: 662.645263671875\n",
      "cost val: 434.49920654296875\n",
      "cost val: 752.5858764648438\n",
      "cost val: 525.4385375976562\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "# e대학원 입학시험 에측\n",
    "#  데이터는 어드미션 csv 이용\n",
    "# 로지스틱 리그레션 수행하누 프리딕션 진행\n",
    "##  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "ad_df = pd.read_csv(\"admission.csv\")\n",
    "ad_df\n",
    "\n",
    "#print(ad_df)\n",
    "\n",
    "x_data = ad_df[[\"gre\", \"gpa\"]].values\n",
    "\n",
    "y_data = ad_df[[\"admit\"]].values\n",
    "\n",
    "#print(ad_df[[\"admit\"]].values)\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "# 함수 만들 때 이렇게 만들어서  저기 logits\n",
    "logit = tf.matmul(X,W) + b\n",
    "# sigmoid 이용 곡선 이동\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# cost func\n",
    "# 매끄러운 곡선을 위해\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits= logit, labels= Y))\n",
    "\n",
    "# train node\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "## session 초기화 (initialization)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={ X : x_data,\n",
    "                                                     Y : y_data})\n",
    "    if step % 300 == 0:\n",
    "        print(\"cost val: {}\".format(cost_val))\n",
    "\n",
    "#predict\n",
    "result = sess.run(H, feed_dict={X : [[340,2.5]]})\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[380.     3.61]\n",
      " [660.     3.67]\n",
      " [800.     4.  ]\n",
      " [640.     3.19]\n",
      " [520.     2.93]\n",
      " [760.     3.  ]\n",
      " [560.     2.98]\n",
      " [400.     3.08]\n",
      " [540.     3.39]\n",
      " [700.     3.92]\n",
      " [800.     4.  ]\n",
      " [440.     3.22]\n",
      " [760.     4.  ]\n",
      " [700.     3.08]\n",
      " [700.     4.  ]\n",
      " [480.     3.44]\n",
      " [780.     3.87]\n",
      " [360.     2.56]\n",
      " [800.     3.75]\n",
      " [540.     3.81]\n",
      " [500.     3.17]\n",
      " [660.     3.63]\n",
      " [600.     2.82]\n",
      " [680.     3.19]\n",
      " [760.     3.35]\n",
      " [800.     3.66]\n",
      " [620.     3.61]\n",
      " [520.     3.74]\n",
      " [780.     3.22]\n",
      " [520.     3.29]\n",
      " [540.     3.78]\n",
      " [760.     3.35]\n",
      " [600.     3.4 ]\n",
      " [800.     4.  ]\n",
      " [360.     3.14]\n",
      " [400.     3.05]\n",
      " [580.     3.25]\n",
      " [520.     2.9 ]\n",
      " [500.     3.13]\n",
      " [520.     2.68]\n",
      " [560.     2.42]\n",
      " [580.     3.32]\n",
      " [600.     3.15]\n",
      " [500.     3.31]\n",
      " [700.     2.94]\n",
      " [460.     3.45]\n",
      " [580.     3.46]\n",
      " [500.     2.97]\n",
      " [440.     2.48]\n",
      " [400.     3.35]\n",
      " [640.     3.86]\n",
      " [440.     3.13]\n",
      " [740.     3.37]\n",
      " [680.     3.27]\n",
      " [660.     3.34]\n",
      " [740.     4.  ]\n",
      " [560.     3.19]\n",
      " [380.     2.94]\n",
      " [400.     3.65]\n",
      " [600.     2.82]\n",
      " [620.     3.18]\n",
      " [560.     3.32]\n",
      " [640.     3.67]\n",
      " [680.     3.85]\n",
      " [580.     4.  ]\n",
      " [600.     3.59]\n",
      " [740.     3.62]\n",
      " [620.     3.3 ]\n",
      " [580.     3.69]\n",
      " [800.     3.73]\n",
      " [640.     4.  ]\n",
      " [300.     2.92]\n",
      " [480.     3.39]\n",
      " [580.     4.  ]\n",
      " [720.     3.45]\n",
      " [720.     4.  ]\n",
      " [560.     3.36]\n",
      " [800.     4.  ]\n",
      " [540.     3.12]\n",
      " [620.     4.  ]\n",
      " [700.     2.9 ]\n",
      " [620.     3.07]\n",
      " [500.     2.71]\n",
      " [380.     2.91]\n",
      " [500.     3.6 ]\n",
      " [520.     2.98]\n",
      " [600.     3.32]\n",
      " [600.     3.48]\n",
      " [700.     3.28]\n",
      " [660.     4.  ]\n",
      " [700.     3.83]\n",
      " [720.     3.64]\n",
      " [800.     3.9 ]\n",
      " [580.     2.93]\n",
      " [660.     3.44]\n",
      " [660.     3.33]\n",
      " [640.     3.52]\n",
      " [480.     3.57]\n",
      " [700.     2.88]\n",
      " [400.     3.31]\n",
      " [340.     3.15]\n",
      " [580.     3.57]\n",
      " [380.     3.33]\n",
      " [540.     3.94]\n",
      " [660.     3.95]\n",
      " [740.     2.97]\n",
      " [700.     3.56]\n",
      " [480.     3.13]\n",
      " [400.     2.93]\n",
      " [480.     3.45]\n",
      " [680.     3.08]\n",
      " [420.     3.41]\n",
      " [360.     3.  ]\n",
      " [600.     3.22]\n",
      " [720.     3.84]\n",
      " [620.     3.99]\n",
      " [440.     3.45]\n",
      " [700.     3.72]\n",
      " [800.     3.7 ]\n",
      " [340.     2.92]\n",
      " [520.     3.74]\n",
      " [480.     2.67]\n",
      " [520.     2.85]\n",
      " [500.     2.98]\n",
      " [720.     3.88]\n",
      " [540.     3.38]\n",
      " [600.     3.54]\n",
      " [740.     3.74]\n",
      " [540.     3.19]\n",
      " [460.     3.15]\n",
      " [620.     3.17]\n",
      " [640.     2.79]\n",
      " [580.     3.4 ]\n",
      " [500.     3.08]\n",
      " [560.     2.95]\n",
      " [500.     3.57]\n",
      " [560.     3.33]\n",
      " [700.     4.  ]\n",
      " [620.     3.4 ]\n",
      " [600.     3.58]\n",
      " [640.     3.93]\n",
      " [700.     3.52]\n",
      " [620.     3.94]\n",
      " [580.     3.4 ]\n",
      " [580.     3.4 ]\n",
      " [380.     3.43]\n",
      " [480.     3.4 ]\n",
      " [560.     2.71]\n",
      " [480.     2.91]\n",
      " [740.     3.31]\n",
      " [800.     3.74]\n",
      " [400.     3.38]\n",
      " [640.     3.94]\n",
      " [580.     3.46]\n",
      " [620.     3.69]\n",
      " [580.     2.86]\n",
      " [560.     2.52]\n",
      " [480.     3.58]\n",
      " [660.     3.49]\n",
      " [700.     3.82]\n",
      " [600.     3.13]\n",
      " [640.     3.5 ]\n",
      " [700.     3.56]\n",
      " [520.     2.73]\n",
      " [580.     3.3 ]\n",
      " [700.     4.  ]\n",
      " [440.     3.24]\n",
      " [720.     3.77]\n",
      " [500.     4.  ]\n",
      " [600.     3.62]\n",
      " [400.     3.51]\n",
      " [540.     2.81]\n",
      " [680.     3.48]\n",
      " [800.     3.43]\n",
      " [500.     3.53]\n",
      " [620.     3.37]\n",
      " [520.     2.62]\n",
      " [620.     3.23]\n",
      " [620.     3.33]\n",
      " [300.     3.01]\n",
      " [620.     3.78]\n",
      " [500.     3.88]\n",
      " [700.     4.  ]\n",
      " [540.     3.84]\n",
      " [500.     2.79]\n",
      " [800.     3.6 ]\n",
      " [560.     3.61]\n",
      " [580.     2.88]\n",
      " [560.     3.07]\n",
      " [500.     3.35]\n",
      " [640.     2.94]\n",
      " [800.     3.54]\n",
      " [640.     3.76]\n",
      " [380.     3.59]\n",
      " [600.     3.47]\n",
      " [560.     3.59]\n",
      " [660.     3.07]\n",
      " [400.     3.23]\n",
      " [600.     3.63]\n",
      " [580.     3.77]\n",
      " [800.     3.31]\n",
      " [580.     3.2 ]\n",
      " [700.     4.  ]\n",
      " [420.     3.92]\n",
      " [600.     3.89]\n",
      " [780.     3.8 ]\n",
      " [740.     3.54]\n",
      " [640.     3.63]\n",
      " [540.     3.16]\n",
      " [580.     3.5 ]\n",
      " [740.     3.34]\n",
      " [580.     3.02]\n",
      " [460.     2.87]\n",
      " [640.     3.38]\n",
      " [600.     3.56]\n",
      " [660.     2.91]\n",
      " [340.     2.9 ]\n",
      " [460.     3.64]\n",
      " [460.     2.98]\n",
      " [560.     3.59]\n",
      " [540.     3.28]\n",
      " [680.     3.99]\n",
      " [480.     3.02]\n",
      " [800.     3.47]\n",
      " [800.     2.9 ]\n",
      " [720.     3.5 ]\n",
      " [620.     3.58]\n",
      " [540.     3.02]\n",
      " [480.     3.43]\n",
      " [720.     3.42]\n",
      " [580.     3.29]\n",
      " [600.     3.28]\n",
      " [380.     3.38]\n",
      " [420.     2.67]\n",
      " [800.     3.53]\n",
      " [620.     3.05]\n",
      " [660.     3.49]\n",
      " [480.     4.  ]\n",
      " [500.     2.86]\n",
      " [700.     3.45]\n",
      " [440.     2.76]\n",
      " [520.     3.81]\n",
      " [680.     2.96]\n",
      " [620.     3.22]\n",
      " [540.     3.04]\n",
      " [800.     3.91]\n",
      " [680.     3.34]\n",
      " [440.     3.17]\n",
      " [680.     3.64]\n",
      " [640.     3.73]\n",
      " [660.     3.31]\n",
      " [620.     3.21]\n",
      " [520.     4.  ]\n",
      " [540.     3.55]\n",
      " [740.     3.52]\n",
      " [640.     3.35]\n",
      " [520.     3.3 ]\n",
      " [620.     3.95]\n",
      " [520.     3.51]\n",
      " [640.     3.81]\n",
      " [680.     3.11]\n",
      " [440.     3.15]\n",
      " [520.     3.19]\n",
      " [620.     3.95]\n",
      " [520.     3.9 ]\n",
      " [380.     3.34]\n",
      " [560.     3.24]\n",
      " [600.     3.64]\n",
      " [680.     3.46]\n",
      " [500.     2.81]\n",
      " [640.     3.95]\n",
      " [540.     3.33]\n",
      " [680.     3.67]\n",
      " [660.     3.32]\n",
      " [520.     3.12]\n",
      " [600.     2.98]\n",
      " [460.     3.77]\n",
      " [580.     3.58]\n",
      " [680.     3.  ]\n",
      " [660.     3.14]\n",
      " [660.     3.94]\n",
      " [360.     3.27]\n",
      " [660.     3.45]\n",
      " [520.     3.1 ]\n",
      " [440.     3.39]\n",
      " [600.     3.31]\n",
      " [800.     3.22]\n",
      " [660.     3.7 ]\n",
      " [800.     3.15]\n",
      " [420.     2.26]\n",
      " [620.     3.45]\n",
      " [800.     2.78]\n",
      " [680.     3.7 ]\n",
      " [800.     3.97]\n",
      " [480.     2.55]\n",
      " [520.     3.25]\n",
      " [560.     3.16]\n",
      " [460.     3.07]\n",
      " [540.     3.5 ]\n",
      " [720.     3.4 ]\n",
      " [640.     3.3 ]\n",
      " [660.     3.6 ]\n",
      " [400.     3.15]\n",
      " [680.     3.98]\n",
      " [220.     2.83]\n",
      " [580.     3.46]\n",
      " [540.     3.17]\n",
      " [580.     3.51]\n",
      " [540.     3.13]\n",
      " [440.     2.98]\n",
      " [560.     4.  ]\n",
      " [660.     3.67]\n",
      " [660.     3.77]\n",
      " [520.     3.65]\n",
      " [540.     3.46]\n",
      " [300.     2.84]\n",
      " [340.     3.  ]\n",
      " [780.     3.63]\n",
      " [480.     3.71]\n",
      " [540.     3.28]\n",
      " [460.     3.14]\n",
      " [460.     3.58]\n",
      " [500.     3.01]\n",
      " [420.     2.69]\n",
      " [520.     2.7 ]\n",
      " [680.     3.9 ]\n",
      " [680.     3.31]\n",
      " [560.     3.48]\n",
      " [580.     3.34]\n",
      " [500.     2.93]\n",
      " [740.     4.  ]\n",
      " [660.     3.59]\n",
      " [420.     2.96]\n",
      " [560.     3.43]\n",
      " [460.     3.64]\n",
      " [620.     3.71]\n",
      " [520.     3.15]\n",
      " [620.     3.09]\n",
      " [540.     3.2 ]\n",
      " [660.     3.47]\n",
      " [500.     3.23]\n",
      " [560.     2.65]\n",
      " [500.     3.95]\n",
      " [580.     3.06]\n",
      " [520.     3.35]\n",
      " [500.     3.03]\n",
      " [600.     3.35]\n",
      " [580.     3.8 ]\n",
      " [400.     3.36]\n",
      " [620.     2.85]\n",
      " [780.     4.  ]\n",
      " [620.     3.43]\n",
      " [580.     3.12]\n",
      " [700.     3.52]\n",
      " [540.     3.78]\n",
      " [760.     2.81]\n",
      " [700.     3.27]\n",
      " [720.     3.31]\n",
      " [560.     3.69]\n",
      " [720.     3.94]\n",
      " [520.     4.  ]\n",
      " [540.     3.49]\n",
      " [680.     3.14]\n",
      " [460.     3.44]\n",
      " [560.     3.36]\n",
      " [480.     2.78]\n",
      " [460.     2.93]\n",
      " [620.     3.63]\n",
      " [580.     4.  ]\n",
      " [800.     3.89]\n",
      " [540.     3.77]\n",
      " [680.     3.76]\n",
      " [680.     2.42]\n",
      " [620.     3.37]\n",
      " [560.     3.78]\n",
      " [560.     3.49]\n",
      " [620.     3.63]\n",
      " [800.     4.  ]\n",
      " [640.     3.12]\n",
      " [540.     2.7 ]\n",
      " [700.     3.65]\n",
      " [540.     3.49]\n",
      " [540.     3.51]\n",
      " [660.     4.  ]\n",
      " [480.     2.62]\n",
      " [420.     3.02]\n",
      " [740.     3.86]\n",
      " [580.     3.36]\n",
      " [640.     3.17]\n",
      " [640.     3.51]\n",
      " [800.     3.05]\n",
      " [660.     3.88]\n",
      " [600.     3.38]\n",
      " [620.     3.75]\n",
      " [460.     3.99]\n",
      " [620.     4.  ]\n",
      " [560.     3.04]\n",
      " [460.     2.63]\n",
      " [700.     3.65]\n",
      " [600.     3.89]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ad_df = pd.read_csv(\"admission.csv\")\n",
    "ad_df\n",
    "\n",
    "#print(ad_df)\n",
    "\n",
    "x_data = ad_df[[\"gre\", \"gpa\"]].values\n",
    "\n",
    "y_data = ad_df[[\"admit\"]].values\n",
    "\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e대학원 입학시험 에측\n",
    "#  데이터는 어드미션 csv 이용\n",
    "# 로지스틱 리그레션 수행하누 프리딕션 진행\n",
    "##  정확도 측정 -> 70% 이상\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admit</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.184434</td>\n",
       "      <td>0.178212</td>\n",
       "      <td>-0.242513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gre</th>\n",
       "      <td>0.184434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384266</td>\n",
       "      <td>-0.123447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpa</th>\n",
       "      <td>0.178212</td>\n",
       "      <td>0.384266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.057461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <td>-0.242513</td>\n",
       "      <td>-0.123447</td>\n",
       "      <td>-0.057461</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          admit       gre       gpa      rank\n",
       "admit  1.000000  0.184434  0.178212 -0.242513\n",
       "gre    0.184434  1.000000  0.384266 -0.123447\n",
       "gpa    0.178212  0.384266  1.000000 -0.057461\n",
       "rank  -0.242513 -0.123447 -0.057461  1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. data loading\n",
    "df = pd.read_csv(\"admission.csv\")\n",
    "#df.head()\n",
    "\n",
    "#상관계수 이용 확인>><<?????????????????????\n",
    "# 랭크 조절 해 (음의 상관계수)\n",
    "# 랭크 값 역순으로 조절\n",
    "df.corr()\n",
    "# 랭크 높으면 높을 수록 합격이 되어야"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPU2_ENV",
   "language": "python",
   "name": "cpu2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
